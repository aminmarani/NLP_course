{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7k0j3i04EiT"
   },
   "source": [
    "# CSE 325/425 NLP\n",
    "### Programming Project 1\n",
    "\n",
    "You are asked to implement basic text pre-procssing and then define and train the Glove model. I removed parts of my implementation and you will need to complete them.\n",
    "\n",
    "Your codes are evaluated based on\n",
    "\n",
    "*   Correct and reasonable text preprocessing.\n",
    "*   Convergence of the model training.\n",
    "*   Speeding up of the word indexing and model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVlyHgOINkT-"
   },
   "source": [
    "**I used gdown and wget command to directly download the dataset. This way we don't need to login to our Google account each time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HNfPb3gzHV8",
    "outputId": "9211c422-8c47-4d85-b408-617c225de7fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-17 20:42:18--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2021-03-17 20:42:18--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2021-03-17 20:42:18--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip.1’\n",
      "\n",
      "glove.6B.zip.1      100%[===================>] 822.24M  5.11MB/s    in 2m 40s  \n",
      "\n",
      "2021-03-17 20:44:58 (5.15 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
      "\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1bR4yPeb6BQIPSQ2p_a8H9Q3o2NlMExZk\n",
      "To: /content/small.csv\n",
      "46.6MB [00:00, 128MB/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# # the following working directory should contain small.csv and glove.6B.300d.txt\n",
    "# os.chdir('/content/drive/My Drive/Teaching/teaching at Lehigh/2021_sp_nlp/Project 1/d\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!gdown --id 1bR4yPeb6BQIPSQ2p_a8H9Q3o2NlMExZk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFf5lEFOOjYd"
   },
   "source": [
    "extracting glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Jm5iJSvBOmH8"
   },
   "outputs": [],
   "source": [
    "#!unzip glove.6B.zip\n",
    "!unzip -p glove.6B.zip glove.6B.300d.txt > glove.6B.300d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbdCMdr6uxlS"
   },
   "source": [
    "## Text Preprocessing and Dataset Construction\n",
    "\n",
    "### Define the WordIndexer class to\n",
    "*   hold the mapping from words to their indices and the indices to words.\n",
    "*   map from a list of sentences to a list of integers so that words are mapped to their indices, in the same order as the original words (except some words replaced).\n",
    "\n",
    "### Inherit from the `torch.utils.data.Dataset` class and create the AmazonReviewGloveDataset class to\n",
    "\n",
    "\n",
    "*   load the Amazon reviews in the csv format. Tokenize the review texts into sentences (a review can contain more than one sentence).\n",
    "*   use the WordIndexer class to obtain the indices of the words in the sentences.\n",
    "*   compute the X (word co-occurrence) matrix as the Glove paper indicates.\n",
    "\n",
    "We provide the function to read the pretrained word vectors from text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFcyggrF0h-6",
    "outputId": "c85a51c5-80d8-415f-807d-c889439aaf30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class WordIndexer:\n",
    "    \"\"\"Transform a dataset of text to a list of index of words.\"\"\"\n",
    "\n",
    "    def __init__(self, min_word_occurences=10, oov_word=\"OOV\"):\n",
    "        \"\"\" min_word_occurrences: integer, the minimum frequency of the word to keep.\n",
    "            oov_word: string, a special string for out-of-vocabulary words.\n",
    "        \"\"\"\n",
    "        self.oov_word = oov_word\n",
    "        self.min_word_occurences = min_word_occurences\n",
    "        # word to integer index mapping\n",
    "        self.word_to_index = {oov_word: 0}\n",
    "        # the inverse of the above mapping\n",
    "        self.index_to_word = [oov_word]\n",
    "        # this is for storing the word frequencies for removing infrequent words\n",
    "        self.word_occurrences = {}\n",
    "        # regular expression for retaining meaningful English words\n",
    "        self.re_words = re.compile(r\"\\b[a-zA-Z]{2,}\\b\")\n",
    "\n",
    "    def get_word_index(self, word, add_new_word = True):\n",
    "        \"\"\" Find the index of a word.\n",
    "                \n",
    "            word: string, the query word.\n",
    "            add_new_word: if true and the word has no entry, assign a new integer index to word.\n",
    "                            if false, return the index of the oov_word\n",
    "        \"\"\"\n",
    "        ### Your codes go here (10 points) ###\n",
    "        if word in self.word_to_index.keys() and add_new_word:\n",
    "          self.word_occurrences[word] += 1\n",
    "          return self.word_to_index[word]\n",
    "        elif word in self.word_to_index.keys():\n",
    "          return self.word_to_index[word]\n",
    "        elif add_new_word:\n",
    "          self.word_to_index[word] = len(self.word_to_index) #update new word_to_index\n",
    "          self.word_occurrences[word] = 1\n",
    "          #update new index_to_word\n",
    "          self.index_to_word.append(word)\n",
    "          return self.word_to_index[word]\n",
    "        else: #if we are not allowed to add a new word, return OOV=0 index\n",
    "          return 0\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def n_words(self):\n",
    "        \"\"\" return: the vocabulary size\n",
    "        \"\"\"\n",
    "        return len(self.word_to_index)\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        \"\"\" texts: list of sentences, each of which is a string\n",
    "            \n",
    "            Split each sentence into a list of words.\n",
    "            Then filter out the infrequent words.\n",
    "            Other text preprocessing, such as\n",
    "                lower-casing,\n",
    "                stop-word removal, and\n",
    "                advance word tokenization\n",
    "                are possible here.\n",
    "            Lastly setup the word-to-index and index-to-word dictionaries.\n",
    "            \n",
    "            return: a list of lists of indices of words in each sentence.\n",
    "                    For example: [[1,2,3], [4,5,6]] where,\n",
    "                        [1,2,3] are the indices of words in the first sentence\n",
    "                        [4,5,6] are the indices of words in the second sentence\n",
    "                    \n",
    "        \"\"\"\n",
    "        text_list = [] #list of lists including tokens of each sentence\n",
    "        #defining a regular expression tokenizer\n",
    "        tokenizer = RegexpTokenizer(self.re_words)\n",
    "\n",
    "        # Step 1: Obtain list of lists of words. Lower-casing and tokenization happen here.\n",
    "        ### Your codes go here (10 points) ###\n",
    "        for t in texts:\n",
    "          #reading stopwords\n",
    "          en_stop = set(stopwords.words('english'))\n",
    "          #lower the selected text and tokenize it\n",
    "          tokens = tokenizer.tokenize(t.lower())\n",
    "          # remove stop words from tokens\n",
    "          stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "          # add tokens to list\n",
    "          text_list.append(stopped_tokens)\n",
    "        \n",
    "\n",
    "\n",
    "        # Step 2: Build a dictionary using the Counter class\n",
    "        # keep the unique words and their counts\n",
    "        # filter out the infrequent ones using the threshold self.min_word_occurences.\n",
    "        # the results is a vocabulary in self.word_to_index and self.index_to_word.\n",
    "        ### Your codes go here (10 points) ###\n",
    "\n",
    "\n",
    "        #removing terms that are below a threshold\n",
    "        for w in self.index_to_word[1:]: #we skip the 'oov' term\n",
    "          if self.word_occurrences[w] < self.min_word_occurences:#we have to delete it from word_occurences, word_to_index, and index_to_word\n",
    "            del self.word_occurrences[w]\n",
    "            del self.word_to_index[w]\n",
    "            self.index_to_word.remove(w)\n",
    "\n",
    "        #going through all sentences\n",
    "        for txt in text_list:\n",
    "          for t in txt:\n",
    "            #updating word_index, index_word, and word_occurences\n",
    "            self.get_word_index(t)\n",
    "\n",
    "\n",
    "        # save the word and their counts to a file.\n",
    "        with open('./train_word_counts.txt', 'w') as out_f:\n",
    "            a = sorted([(word, count) for word, count in self.word_occurrences.items()],\n",
    "                   key = lambda x:x[1], reverse=True)\n",
    "            for word, count in a:\n",
    "                out_f.write('{}:{}\\n'.format(word, count))\n",
    "\n",
    "        # Step 3: build and return the corpus in index representation\n",
    "        # using the vocabulary built in the last step.\n",
    "        # Be careful about words that are not in the vocabulary.\n",
    "        ### Your codes go here (10 points) ###\n",
    "\n",
    "        \n",
    "        \n",
    "        #return self.word_occurrences,self.word_to_index, self.index_to_word\n",
    "        ind_list = []\n",
    "\n",
    "        for txt in text_list:\n",
    "            ind_list.append([self.word_to_index[t] for t in txt if t in self.index_to_word])\n",
    "\n",
    "        return ind_list\n",
    "    \n",
    "class AmazonReviewGloveDataset(Dataset):\n",
    "    def __init__(self, path, right_window = 4, min_word_occurences = 10):\n",
    "        \"\"\" Load the reviews from a csv file. One row is one review.\n",
    "                \n",
    "            path: path to the csv file containing the reviews and their ratings\n",
    "            right_window: integer, how large the window is to get context words.\n",
    "            min_word_occurrences: integer, the minimum frequency of the word to keep.\n",
    "\n",
    "            No return value\n",
    "        \"\"\"\n",
    "        self.right_window = right_window\n",
    "        \n",
    "        # Step 1: tokenize the first field of each row in the csv file into sentences\n",
    "        #         (e.g. using nltk.tokenize.sent_tokenize).\n",
    "        #           Use pandas.read_csv to load the given training csv file.\n",
    "        df = pd.read_csv(path)\n",
    "        texts = []  # each element of texts is a single sentence.\n",
    "        ### Your codes go here (10 points) ###\n",
    "        #defining a regular expression tokenizer\n",
    "        tokenizer = RegexpTokenizer(re.compile(r\"\\b[a-zA-Z]{2,}\\b\"))\n",
    "        \n",
    "        #tokenization and removing stop words\n",
    "        for t in df.reviewText:\n",
    "          #reading stopwords\n",
    "          en_stop = set(stopwords.words('english'))\n",
    "          #if text field is empty\n",
    "          if pd.isna(t) or pd.isnull(t):\n",
    "            continue\n",
    "          #lower the selected text and tokenize it\n",
    "          tokens = tokenizer.tokenize(t.lower())\n",
    "          # remove stop words from tokens\n",
    "          stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "          # add tokens to list ==> since the WordIndexer.fit() recieves a list of sentences (not list of lists) then we convert the tokens list to a sentence\n",
    "          texts.append(' '.join(stopped_tokens))\n",
    "        \n",
    "\n",
    "        print ('{} reviews loaded. {} sentences.'.format(df.shape[0], len(texts)))\n",
    "        \n",
    "        \n",
    "        # Step 2: pass the list of all sentences from step 1 (texts) to WordIndexer.\n",
    "        # Use its fit_transform function to turn list of sentences into list of lists of word indices in the sentences.\n",
    "        # Keep the word ordering.\n",
    "        print ('Indexing the corpus...')\n",
    "        self.indexer = WordIndexer(min_word_occurences=min_word_occurences)\n",
    "        corpus = self.indexer.fit_transform(texts)\n",
    "        print ('Done indexing the corpus.')\n",
    "\n",
    "        \n",
    "        \n",
    "        # Step 3: go through the results (corpus) from step 2 and gather (center, context) in comatrix,\n",
    "        # which is a collections.Counter object.\n",
    "        # In the Counter, keys are (center, context) pairs\n",
    "        # values are the number of their co-occurrence as defined in the Glove paper.\n",
    "        print ('Constructing the co-occurrence matrix...')\n",
    "        ### Your codes go here (10 points) ###\n",
    "        dic = dict()#creat an empty dictionary\n",
    "        for doc in corpus:\n",
    "          for i in range(len(doc)):#this is the center word\n",
    "            for j in range(max(0,int(i-right_window/2)),min(len(doc),int(i+right_window/2))): #make sure we don't get outside the vector\n",
    "              if i != j: #just different words\n",
    "                if tuple(sorted([doc[i],doc[j]])) in dic.keys():\n",
    "                  dic[tuple(sorted([doc[i],doc[j]]))] += 1\n",
    "                else:\n",
    "                  dic[tuple(sorted([doc[i],doc[j]]))] = 1\n",
    "\n",
    "        comatrix = Counter(dic)\n",
    "        \n",
    "        print ('Done making the co-occurrence matrix...')\n",
    "\n",
    "\n",
    "        # save the comatrix to file                    \n",
    "        with open('./comatrix.pkl', 'wb') as out_f:\n",
    "            pickle.dump(comatrix, out_f)\n",
    "\n",
    "        # Step 4: flatten the co-occurrence matrix and store the center, context, and X_ij\n",
    "        # in three lists: self.left (center word), self.right (context word), self.n_occurrences (X_ij)\n",
    "        #self.left, self.right, self.n_occurrences = None, None, None\n",
    "        ### Your codes go here (10 points) ###\n",
    "        self.left = list(np.array(list(comatrix.keys()))[:,0])\n",
    "        self.right = list(np.array(list(comatrix.keys()))[:,1])\n",
    "        self.n_occurrences = list(comatrix.values())\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.left[index], self.right[index], self.n_occurrences[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.left)\n",
    "    \n",
    "def load_pretrained_wv(path):\n",
    "    \"\"\"\n",
    "        Load the pretrained word vectors downloaded from Stanford NLP.\n",
    "    \"\"\"\n",
    "    wv = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            items = line.split(' ')\n",
    "            wv[items[0]] = torch.DoubleTensor([float(a) for a in items[1:]])\n",
    "    return wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ2q4gbfNHfT"
   },
   "source": [
    "## Define the Glove model\n",
    "The parameters include\n",
    "\n",
    "*   Vectors of words when used as center and context words\n",
    "\n",
    "The parameters are defined for you already and please don't change the variable names.\n",
    "\n",
    "There is an option to pass in pre-trained word vectors to replace random initialization of the word vectors in this model.\n",
    "\n",
    "You have to complete the forward function to compute the predictions of log X_ij.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Lzv3KCwHBrVv"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.functional as F\n",
    "import torch\n",
    "\n",
    "class GloveModel(nn.Module):\n",
    "    def __init__(self, word_indexer, wv = None, word_dims = 300, BASE_STD = 0.01, random_state = 0):\n",
    "        \"\"\" Specify and initialize the parameters of the Glove network.\n",
    "        \"\"\"\n",
    "        super(GloveModel, self).__init__()\n",
    "        num_words = word_indexer.n_words\n",
    "        \n",
    "        torch.manual_seed(random_state)\n",
    "        \n",
    "        # initialize left and right word vectors\n",
    "        self.L_vecs = (torch.randn((num_words, word_dims))  * BASE_STD)\n",
    "        self.R_vecs = (torch.randn((num_words, word_dims))  * BASE_STD)\n",
    "       \n",
    "        if wv is not None:\n",
    "            num_replaced = 0\n",
    "            for i in range(num_words):\n",
    "                word = word_indexer.index_to_word[i]\n",
    "                if word in wv:\n",
    "                    num_replaced += 1\n",
    "                    self.L_vecs[i] = wv[word]\n",
    "                    self.R_vecs[i] = wv[word]\n",
    "            print (f'Replaced {float(num_replaced) / num_words}')\n",
    "            \n",
    "        self.L_vecs.requires_grad_()\n",
    "        self.R_vecs.requires_grad_()\n",
    "        \n",
    "        # gather the trainable parameters\n",
    "        self.parameters = [self.L_vecs, self.R_vecs]\n",
    "        \n",
    "    def forward(self, left_indices, right_indices):\n",
    "        \"\"\" Implement w_i^t w_j (the left-hand-side of Eq. (16) in the Glove paper)\n",
    "        \n",
    "            left_indices: torch.Tensor, a batch of center words\n",
    "            right_indices: torch.Tensor, a batch of context words, of the same shape of left_indices.\n",
    "            \n",
    "            left_indices[i] and right_indices[i] is the i-th pair in the training data.\n",
    "            \n",
    "            return: torch.Tensor of the same shape of left_indices\n",
    "        \"\"\"\n",
    "        ###my notes to define w_i^t and w_j\n",
    "        ###1. p_ij = P(j|i) = X_ij/X_i (X_ij: number of times word i was center and word j was context, Xi: Sum (X_ik)) ==> paragraph 2 section 3\n",
    "        ###2. w_i^t * w_j = log(p_ij) = log(X_ij) - log(X_i)\n",
    "        ### Your codes go here (10 points) ###\n",
    "        #print(self.L_vecs.shape)\n",
    "        return torch.sum((model.L_vecs[left_indices]*(model.R_vecs[right_indices])),axis=1)\n",
    "\n",
    "\n",
    "        #initialize the return array\n",
    "        # res = torch.zeros(left_indices.shape,dtype=torch.double)\n",
    "\n",
    "        # for i in range(len(left_indices)):\n",
    "        #   res[i] = ? / num_words[left_indices[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1goimxBiNRDi"
   },
   "source": [
    "## Model training, validating, and saving\n",
    "\n",
    "### First define some constants\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCvtEVViNF8D",
    "outputId": "260e789e-d266-461b-fa2a-cb2bd63a77bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./glove.6B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# this will automatically place all tensor on GPU with type Double.\n",
    "# if you are not running on GPU, change this line to\n",
    "# torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "torch.set_default_tensor_type('torch.cuda.DoubleTensor')\n",
    "#torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "\n",
    "# set up a couple of parameters and hyper-parameters\n",
    "\n",
    "# number of epoches to train the model\n",
    "NUM_EPOCH = 25\n",
    "# size of mini-batches\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# dimension of word vectors. The integer should be the same as the dimension of\n",
    "# pretrained word vectors.\n",
    "NUM_DIMS = 300\n",
    "\n",
    "# how to many words to the right to pair with the center word\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "# two hyper-parameters in Eq. (9) of the paper\n",
    "x_max = 100\n",
    "alpha = 0.75\n",
    "\n",
    "# input file containing Amazon review texts.\n",
    "train_path = './small.csv'\n",
    "\n",
    "# where your model is saved.\n",
    "save_path = './glove_model_{}.pt'\n",
    "\n",
    "# optional word vectors pretrained\n",
    "pretrained_wv = './glove.6B.{}d.txt'.format(NUM_DIMS)\n",
    "print (pretrained_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mn1Hz3QSqaCv"
   },
   "outputs": [],
   "source": [
    "# load pretrained word vectors\n",
    "wv = load_pretrained_wv(pretrained_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EnFqhAwurmCt",
    "outputId": "de513416-73b5-47c8-b81a-24a52243f032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3602e-01, -1.1594e-01, -1.7078e-02, -2.9256e-01,  1.6149e-02,\n",
      "         8.6472e-02,  1.5759e-03,  3.4395e-01,  2.1661e-01, -2.1366e+00,\n",
      "         3.5278e-01, -2.3909e-01, -2.2174e-01,  3.6413e-01, -4.5021e-01,\n",
      "         1.2104e-01, -1.5596e-01, -3.8906e-02, -2.9419e-03,  1.6009e-02,\n",
      "        -1.1620e-01,  3.8680e-01,  3.5109e-01,  9.7426e-02, -1.2425e-02,\n",
      "        -1.7864e-01, -2.3259e-01, -2.6960e-01,  4.1083e-02, -7.6194e-02,\n",
      "        -2.3362e-01,  2.0919e-01, -2.7264e-01,  5.4967e-02, -1.8055e+00,\n",
      "         5.6348e-01, -1.2778e-01,  2.3147e-01, -5.8820e-03, -2.6630e-01,\n",
      "         4.1187e-01, -3.7162e-01, -2.0600e-01, -1.9619e-01, -4.3945e-03,\n",
      "         1.2513e-01,  4.6638e-01,  4.5159e-01, -1.5000e-01,  5.9589e-03,\n",
      "         5.9070e-02, -4.1440e-01,  6.1035e-02, -2.1117e-01, -4.0988e-01,\n",
      "         5.6393e-01,  2.3021e-01,  2.7240e-01,  4.9364e-02,  1.4239e-01,\n",
      "         4.1841e-01, -1.3983e-01,  3.4826e-01, -1.0745e-01, -2.5002e-01,\n",
      "        -3.2554e-01,  3.3343e-01, -3.5617e-01,  2.0442e-01,  1.4439e-01,\n",
      "        -1.2686e-01, -7.7273e-02, -1.9667e-01,  1.0759e-01, -1.1860e-01,\n",
      "        -2.5083e-01,  1.4205e-02,  2.7251e-01, -2.3707e-01, -2.3545e-01,\n",
      "        -1.5887e-01,  1.3151e-01,  6.9564e-01,  2.2766e-01,  1.8526e-01,\n",
      "         1.5743e-01, -1.5018e-01, -1.8177e-01, -3.3527e-02, -3.3092e-01,\n",
      "        -2.5205e-01,  5.0913e-01, -2.5607e-01, -5.3686e-01,  1.3397e-01,\n",
      "         6.7046e-02, -9.4473e-02, -2.2270e-01, -3.1469e-01,  8.5932e-02,\n",
      "        -4.3032e-02, -2.5821e-01, -9.5062e-02, -1.8497e-01,  5.8890e-02,\n",
      "         1.8972e-01, -1.7366e-01,  2.5263e-01, -5.4361e-01, -3.7248e-01,\n",
      "        -4.6661e-02, -4.1657e-01, -1.7549e-03, -4.8404e-01,  4.2090e-01,\n",
      "        -1.2749e-03,  9.4697e-03, -1.3380e-01,  7.2351e-02, -1.2096e-01,\n",
      "        -7.2870e-02, -1.8333e-01,  3.9652e-01,  1.1329e-01, -6.3029e-02,\n",
      "        -1.9702e-03,  4.2848e-01,  3.1790e-01, -1.5079e-01,  2.0405e-01,\n",
      "         2.1828e-01,  2.6067e-02,  4.3621e-02,  3.9224e-03, -2.6629e-01,\n",
      "        -2.8312e-01,  5.0497e-02, -1.8993e-01,  1.8996e-01,  2.9517e-01,\n",
      "        -1.1566e-01,  4.0967e-01,  2.2221e-01, -3.9778e-01, -3.3177e-01,\n",
      "        -1.3884e-01, -1.6829e-01, -2.0355e-01, -2.7687e-01, -1.1087e-01,\n",
      "        -6.7466e-01, -1.8108e-01,  1.8512e-01, -9.4616e-02,  1.7856e-01,\n",
      "        -6.6997e-02,  1.1379e-01, -9.3380e-02,  5.6860e-01, -1.3365e-01,\n",
      "         3.4636e-01, -4.1953e-01,  1.7547e-01, -2.4277e-02, -1.2441e-01,\n",
      "         9.2129e-02, -1.6702e-01, -1.4285e-01,  3.1646e-01,  3.0337e-01,\n",
      "         1.4840e-01, -6.7837e-03, -1.0509e+00,  2.2329e-01,  7.5211e-02,\n",
      "         4.4379e-02, -8.5929e-02, -1.1806e-01, -1.6632e-01, -7.8650e-02,\n",
      "         2.6374e-01, -2.2052e-01,  4.5582e-01, -1.5291e-01,  6.2617e-02,\n",
      "        -1.5588e-01,  8.2398e-02, -6.8462e-02, -2.4569e-01,  2.3439e-01,\n",
      "        -3.8633e-01,  2.4835e-01,  2.5334e-01, -2.1189e-01,  4.1494e-03,\n",
      "        -4.3762e-01, -1.3426e-01, -2.4583e-01,  1.4213e-01, -3.3973e-01,\n",
      "         1.4643e+00,  1.6414e-01,  2.2135e-01,  7.4099e-03, -5.5141e-02,\n",
      "        -2.7403e-02,  3.2928e-02,  1.4289e-01, -1.0049e-01, -2.2066e-01,\n",
      "        -3.0380e-01,  6.0624e-02, -1.2408e-01, -5.4114e-01,  2.4374e-01,\n",
      "         8.0903e-02, -7.8264e-02,  8.0091e-02,  9.8551e-03, -2.3077e-01,\n",
      "         1.6006e-01,  6.4075e-02, -4.1613e-01,  2.0494e-01, -1.8681e-01,\n",
      "         3.5367e-02,  2.1759e-01, -8.7823e-02,  3.5452e-01,  1.9578e-01,\n",
      "        -1.5127e-01, -1.0545e-01,  3.5650e-01, -3.8677e-01, -6.3172e-02,\n",
      "         3.1534e-01, -1.5887e-01, -3.1267e-01, -1.7893e-01,  4.1952e-01,\n",
      "         2.3261e-01,  2.0943e-01,  2.7013e-02,  1.7388e-02, -5.9857e-01,\n",
      "        -1.9622e-01, -2.3672e-01,  3.0032e-01,  4.6926e-02, -8.5768e-02,\n",
      "         3.6539e-01, -5.2476e-01, -1.3618e-01,  1.0868e-01,  4.6307e-01,\n",
      "         3.8502e-01,  7.6317e-04, -3.8196e-01,  7.9772e-02, -4.1744e-02,\n",
      "         4.7625e-02, -4.1018e-02,  1.7601e-01,  2.4893e-01, -1.0753e-01,\n",
      "         3.1935e-01, -1.2762e-01, -3.5059e-01,  3.5689e-04,  9.3515e-03,\n",
      "        -8.8616e-02, -3.2785e-01,  9.2063e-02, -6.1405e-02,  2.9053e-01,\n",
      "         2.2404e-02, -1.6879e+00,  2.6712e-01,  3.3419e-01, -5.2533e-02,\n",
      "        -1.9741e-01,  1.3709e-01, -5.4288e-02,  5.6423e-01,  1.9384e-01,\n",
      "         1.7229e-01,  2.9025e-01, -1.6124e-01,  5.9489e-02, -3.1884e-01,\n",
      "        -2.8343e-01,  6.4321e-02, -4.1589e-01, -7.0528e-02,  1.2410e-02,\n",
      "        -4.0208e-01, -2.4963e-01, -3.3760e-01,  7.0098e-02,  2.4642e-01],\n",
      "       device='cpu')\n"
     ]
    }
   ],
   "source": [
    "print (wv['good'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z39JUijCxi4i"
   },
   "source": [
    "### Then define the training, validation, and test data.\n",
    "*   Use the AmazonReviewGloveDataset class to read train dataset.\n",
    "*   Define DataLoader wrapping around the Dataset objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A77sOk99H-nv",
    "outputId": "cb8f452c-cc95-4c65-c0ad-22513c11bf9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 reviews loaded. 99946 sentences.\n",
      "Indexing the corpus...\n",
      "Done indexing the corpus.\n",
      "Constructing the co-occurrence matrix...\n",
      "Done making the co-occurrence matrix...\n"
     ]
    }
   ],
   "source": [
    "# load text data and turn them into a DataLoader object.\n",
    "train_dataset = AmazonReviewGloveDataset(train_path, right_window = WINDOW_SIZE)\n",
    "train_iter = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True) #train_iter[BATCH_SIZE*3] (3: center/left, context/right, co-occurence matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFfzclO7yOwU"
   },
   "source": [
    "### Third, start training.\n",
    "\n",
    "*   You're required to use GPU to train the network, since GPU are ubiquitous (colab or SandBox).\n",
    "*   Complete the function train_and_validate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RBqcAinANLUn",
    "outputId": "1730ea22-d34e-4eb4-81ef-b38a0083ce23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced 0.5851441501189872\n",
      "Training epoch = 0, epoch loss = 422.0002198613877\n",
      "./glove_model_0.pt\n",
      "Training epoch = 1, epoch loss = 67.7488372771832\n",
      "./glove_model_1.pt\n",
      "Training epoch = 2, epoch loss = 32.79253457134109\n",
      "./glove_model_2.pt\n",
      "Training epoch = 3, epoch loss = 21.351249648110656\n",
      "./glove_model_3.pt\n",
      "Training epoch = 4, epoch loss = 15.585608007738987\n",
      "./glove_model_4.pt\n",
      "Training epoch = 5, epoch loss = 12.229513207900908\n",
      "./glove_model_5.pt\n",
      "Training epoch = 6, epoch loss = 10.302480508814023\n",
      "./glove_model_6.pt\n",
      "Training epoch = 7, epoch loss = 9.057496630404547\n",
      "./glove_model_7.pt\n",
      "Training epoch = 8, epoch loss = 8.349369754851022\n",
      "./glove_model_8.pt\n",
      "Training epoch = 9, epoch loss = 7.815144486075659\n",
      "./glove_model_9.pt\n",
      "Training epoch = 10, epoch loss = 7.4634724079793635\n",
      "./glove_model_10.pt\n",
      "Training epoch = 11, epoch loss = 7.177587084865064\n",
      "./glove_model_11.pt\n",
      "Training epoch = 12, epoch loss = 6.9454462134133825\n",
      "./glove_model_12.pt\n",
      "Training epoch = 13, epoch loss = 6.752577898525866\n",
      "./glove_model_13.pt\n",
      "Training epoch = 14, epoch loss = 6.616321362596065\n",
      "./glove_model_14.pt\n",
      "Training epoch = 15, epoch loss = 6.452338359882186\n",
      "./glove_model_15.pt\n",
      "Training epoch = 16, epoch loss = 6.355430346370807\n",
      "./glove_model_16.pt\n",
      "Training epoch = 17, epoch loss = 6.239054708430348\n",
      "./glove_model_17.pt\n",
      "Training epoch = 18, epoch loss = 6.164023545187464\n",
      "./glove_model_18.pt\n",
      "Training epoch = 19, epoch loss = 6.067659836325585\n",
      "./glove_model_19.pt\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "# decide whether to use cpu or gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# initialize the Glove model\n",
    "model = GloveModel(train_dataset.indexer, wv, word_dims = NUM_DIMS)\n",
    "\n",
    "# make sure you use weight_decay to activate the L2 regularization\n",
    "optimizer = torch.optim.Adam(model.parameters, weight_decay=1e-8)\n",
    "\n",
    "x_max = 100 #as told in the paper\n",
    "alpha = 3/4 #as told in the paper\n",
    "\n",
    "def train_and_validate(train_iter):\n",
    "    best_loss = -1\n",
    "    best_epoch = -1\n",
    "    to_save = {}\n",
    "    #a = time.time()\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = len(train_iter)\n",
    "        for l, r, n_lr in train_iter:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Implement the loss function in Eq. (16) of the paper, in three steps.\n",
    "            # Step 1. find the prediction of log(X_ij) using the model \n",
    "            ### Your codes go here (3 points) ###\n",
    "            log_X_ij = torch.log(n_lr)\n",
    "\n",
    "\n",
    "            # Step 2. compute the weights f(X_ij). See Eq. (9) of the Glove paper.\n",
    "            ### Your codes go here (3 points) ###\n",
    "            f_X_ij = torch.where(n_lr>x_max,1.0,(n_lr.type(torch.double)/x_max)**alpha)\n",
    "\n",
    "\n",
    "            # Step 3. compute the loss in Eq. (16) using the predictions and the weights\n",
    "            ### Your codes go here (4 points) ###\n",
    "            wi_wk = model.forward(l,r)\n",
    "\n",
    "            #compute J^ with summation over what we found in all 3 steps above\n",
    "            loss = sum(f_X_ij * (wi_wk - log_X_ij)**2)\n",
    "\n",
    "\n",
    "            # tracking the averaged loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # gradient descent, don't change the following two lines\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Training epoch = {epoch}, epoch loss = {epoch_loss / num_batches}')\n",
    "        #print('time elapsed for this epoch: ',time.time()-a)\n",
    "\n",
    "        # record the model state_dict() for saving later\n",
    "        to_save = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict()\n",
    "        }\n",
    "        torch.save(to_save, save_path.format(epoch))\n",
    "        print (save_path.format(epoch))\n",
    "    \n",
    "#train_and_validate(train_iter, valid_iter = None)\n",
    "train_and_validate(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHlPwSW91FGO"
   },
   "source": [
    "doing some fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPAs6x2kqUMY"
   },
   "source": [
    "## Retrieve similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWTYckiiqT3n",
    "outputId": "9d006d87-52bb-474c-e185-5d80530088be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar terms to phone\n",
      "phone: 0.9999999999999992\n",
      "one: 0.7841699928736577\n",
      "also: 0.7784242563569094\n",
      "get: 0.776436545420805\n",
      "work: 0.7720169614747961\n",
      "well: 0.7672429037411981\n",
      "like: 0.76204903247093\n",
      "make: 0.7560547697164982\n",
      "use: 0.7530570927625488\n",
      "time: 0.7492657199592052\n",
      "------------------------\n",
      "similar terms to case\n",
      "case: 1.0\n",
      "well: 0.7930504356262487\n",
      "really: 0.7886036476869468\n",
      "also: 0.7827657922339956\n",
      "little: 0.7769023708285877\n",
      "one: 0.7732664392797596\n",
      "iphone: 0.7620490449033462\n",
      "fits: 0.7561927765454207\n",
      "however: 0.7546629667775968\n",
      "cases: 0.7545293313955017\n",
      "------------------------\n",
      "similar terms to battery\n",
      "battery: 0.9999999999999998\n",
      "charger: 0.7636414699447694\n",
      "charge: 0.7523930448584852\n",
      "iphone: 0.6969315357804844\n",
      "batteries: 0.6943718998151571\n",
      "charging: 0.690059770471301\n",
      "work: 0.6770834560385134\n",
      "use: 0.6684644499030612\n",
      "usb: 0.6654377780021089\n",
      "case: 0.6649488011869236\n",
      "------------------------\n",
      "similar terms to headset\n",
      "headset: 0.9999999999999998\n",
      "bluetooth: 0.7862720872796206\n",
      "sound: 0.7195849713745094\n",
      "phone: 0.6954760384998393\n",
      "plantronics: 0.6853623337424545\n",
      "headsets: 0.6779393256183799\n",
      "ear: 0.6712019561982543\n",
      "jabra: 0.668642446988339\n",
      "headphones: 0.6611216562374442\n",
      "device: 0.6605560194007886\n",
      "------------------------\n",
      "similar terms to charger\n",
      "charger: 1.0000000000000004\n",
      "usb: 0.7761107503509209\n",
      "battery: 0.7636414699447694\n",
      "charge: 0.7113693652984158\n",
      "iphone: 0.7086222307747397\n",
      "charging: 0.7049421391143702\n",
      "plug: 0.69966820599537\n",
      "samsung: 0.6759318170611813\n",
      "adapter: 0.6739691910631649\n",
      "chargers: 0.6692402509086927\n",
      "------------------------\n",
      "similar terms to quality\n",
      "quality: 0.9999999999999998\n",
      "good: 0.7904808881482684\n",
      "well: 0.7674847136293383\n",
      "price: 0.7525960489059771\n",
      "great: 0.7488457533612983\n",
      "product: 0.7362036366311764\n",
      "work: 0.7296744709074944\n",
      "really: 0.7268222030064018\n",
      "like: 0.7116451945440495\n",
      "however: 0.7064435876093352\n",
      "------------------------\n",
      "similar terms to screen\n",
      "screen: 0.9999999999999999\n",
      "get: 0.7472270741679528\n",
      "phone: 0.7306399139745056\n",
      "one: 0.7290949719694674\n",
      "also: 0.7166629142424924\n",
      "well: 0.7161205025245873\n",
      "protector: 0.7152517896989043\n",
      "like: 0.7044510876514641\n",
      "little: 0.703490890860627\n",
      "really: 0.7034522098051054\n",
      "------------------------\n",
      "similar terms to bluetooth\n",
      "bluetooth: 0.9999999999999997\n",
      "headset: 0.7862720872796206\n",
      "headsets: 0.7066936350459642\n",
      "plantronics: 0.6588342426383348\n",
      "phone: 0.6428874085808787\n",
      "motorola: 0.6384785171458425\n",
      "iphone: 0.6381087841985773\n",
      "battery: 0.6333677555693612\n",
      "sound: 0.630229424150328\n",
      "device: 0.6221353222111091\n",
      "------------------------\n",
      "similar terms to price\n",
      "price: 1.0000000000000007\n",
      "quality: 0.7525960489059771\n",
      "well: 0.742964675547818\n",
      "recommend: 0.7313066338750253\n",
      "great: 0.7260491342215998\n",
      "time: 0.725749482201164\n",
      "good: 0.7151595943178318\n",
      "really: 0.7143974404046689\n",
      "case: 0.7103601533894188\n",
      "product: 0.7100328852106869\n",
      "------------------------\n",
      "similar terms to device\n",
      "device: 0.9999999999999994\n",
      "use: 0.7915002654457748\n",
      "also: 0.7615894702140429\n",
      "using: 0.756219776934856\n",
      "well: 0.7381674370907615\n",
      "devices: 0.7275222379019933\n",
      "get: 0.7237171564618254\n",
      "need: 0.7218738118786945\n",
      "much: 0.7194928146154616\n",
      "even: 0.7094821482369765\n",
      "------------------------\n",
      "similar terms to great\n",
      "great: 0.9999999999999999\n",
      "like: 0.8153040946519092\n",
      "well: 0.8009516809024732\n",
      "work: 0.7958273296407353\n",
      "one: 0.781148772919588\n",
      "good: 0.7681937110408633\n",
      "really: 0.7570982621567771\n",
      "quality: 0.7488457533612983\n",
      "works: 0.74674143064254\n",
      "time: 0.7315339220425322\n",
      "------------------------\n",
      "similar terms to good\n",
      "good: 0.9999999999999998\n",
      "like: 0.7926389126600015\n",
      "quality: 0.7904808881482684\n",
      "great: 0.7681937110408633\n",
      "work: 0.7560184411156515\n",
      "look: 0.7503753668779122\n",
      "pretty: 0.7436771862081482\n",
      "well: 0.7434336272588926\n",
      "really: 0.74303793885273\n",
      "however: 0.7270768026288225\n",
      "------------------------\n",
      "similar terms to well\n",
      "well: 0.9999999999999999\n",
      "also: 0.8460531830649045\n",
      "however: 0.8255730003043553\n",
      "work: 0.823578139076545\n",
      "use: 0.8207134694202952\n",
      "really: 0.8205047504297224\n",
      "get: 0.8167221163134533\n",
      "one: 0.8122321619795634\n",
      "works: 0.8112759775180333\n",
      "would: 0.8060703813357531\n",
      "------------------------\n",
      "similar terms to works\n",
      "works: 1.0000000000000002\n",
      "well: 0.8112759775180333\n",
      "use: 0.7567140262465828\n",
      "work: 0.7478059696484609\n",
      "great: 0.74674143064254\n",
      "iphone: 0.7362419757426825\n",
      "also: 0.7347992529813823\n",
      "however: 0.7331026640179286\n",
      "much: 0.7324308011812433\n",
      "really: 0.7308138003098109\n",
      "------------------------\n",
      "similar terms to better\n",
      "better: 0.9999999999999996\n",
      "much: 0.8044034052302806\n",
      "well: 0.7568309036539795\n",
      "get: 0.7291429463762686\n",
      "also: 0.7262558770615279\n",
      "really: 0.7245542250656019\n",
      "like: 0.7244997244699203\n",
      "would: 0.721605439385637\n",
      "even: 0.7206462925286427\n",
      "think: 0.7174830455160895\n",
      "------------------------\n",
      "similar terms to little\n",
      "little: 1.0000000000000004\n",
      "really: 0.8199679102090499\n",
      "bit: 0.799022214612663\n",
      "get: 0.7893706456020979\n",
      "much: 0.7837294363577426\n",
      "case: 0.7769023708285877\n",
      "thing: 0.7709449613812139\n",
      "also: 0.7666935028828239\n",
      "well: 0.7630828781284523\n",
      "like: 0.7456928456970209\n",
      "------------------------\n",
      "similar terms to easy\n",
      "easy: 0.9999999999999996\n",
      "well: 0.7827416746188045\n",
      "get: 0.7554489413949583\n",
      "also: 0.7467909022552988\n",
      "use: 0.7440531195989784\n",
      "really: 0.7421505429379835\n",
      "one: 0.7166783319850943\n",
      "like: 0.7140562756127333\n",
      "pretty: 0.7133964221509432\n",
      "make: 0.7120084318241507\n",
      "------------------------\n",
      "similar terms to nice\n",
      "nice: 1.0\n",
      "looks: 0.7297278610712005\n",
      "good: 0.7253549991912402\n",
      "really: 0.7239640283652963\n",
      "perfect: 0.7235450297756228\n",
      "case: 0.7189308037300415\n",
      "great: 0.7161383694655881\n",
      "easy: 0.7111607823279115\n",
      "well: 0.7014292228337138\n",
      "pretty: 0.7005096346736445\n",
      "------------------------\n",
      "similar terms to new\n",
      "new: 1.0000000000000002\n",
      "time: 0.7267015739809233\n",
      "would: 0.7089826976968575\n",
      "one: 0.7085475150745073\n",
      "get: 0.6999652642862789\n",
      "still: 0.6977010669691842\n",
      "well: 0.6949875543048575\n",
      "however: 0.6938009378051923\n",
      "got: 0.6924179821350323\n",
      "first: 0.6871464442020397\n",
      "------------------------\n",
      "similar terms to long\n",
      "long: 1.0000000000000002\n",
      "time: 0.7246025760149887\n",
      "last: 0.7245862895301187\n",
      "use: 0.7060020781158016\n",
      "well: 0.7044443466598619\n",
      "would: 0.6996240904773329\n",
      "make: 0.6947413556616312\n",
      "get: 0.6916017361648795\n",
      "much: 0.6874543034236283\n",
      "way: 0.6834864902731782\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "test_aspect_words = ['phone', 'case', 'battery', 'headset', 'charger', 'quality', 'screen', 'bluetooth', 'price', 'device']\n",
    "test_sentimental_words = ['great', 'good', 'well', 'works', 'better', 'little', 'easy', 'nice', 'new', 'long']\n",
    "\n",
    "#glove = load_model(save_path.format(0), train_dataset.indexer)\n",
    "glove = model\n",
    "avg_word_vectors = (glove.L_vecs.to('cpu') + glove.R_vecs.to('cpu')) / 2\n",
    "avg_word_vectors = avg_word_vectors.detach().numpy()\n",
    "\n",
    "n_words = train_dataset.indexer.n_words\n",
    "\n",
    "row_normalized = normalize(avg_word_vectors)\n",
    "#sim = row_normalized.dot(row_normalized.T)\n",
    "\n",
    "for w in test_aspect_words:\n",
    "    w_idx = train_dataset.indexer.word_to_index[w]\n",
    "    l = []\n",
    "    for i in range(n_words):\n",
    "        #l.append((i, sim[w_idx, i]))\n",
    "        l.append((i, row_normalized[w_idx].dot(row_normalized[i].T)))\n",
    "    l = sorted(l, key = lambda x:x[1], reverse = True)\n",
    "    print('similar terms to {0}'.format(w))\n",
    "    for i in range(10):\n",
    "        print (f'{train_dataset.indexer.index_to_word[l[i][0]]}: {l[i][1]}')\n",
    "    print('------------------------')\n",
    "        \n",
    "for w in test_sentimental_words:\n",
    "    w_idx = train_dataset.indexer.word_to_index[w]\n",
    "    l = []\n",
    "    for i in range(n_words):\n",
    "        #l.append((i, sim[w_idx, i]))\n",
    "        l.append((i, row_normalized[w_idx].dot(row_normalized[i].T)))\n",
    "    l = sorted(l, key = lambda x:x[1], reverse = True)\n",
    "    print('similar terms to {0}'.format(w))\n",
    "    for i in range(10):\n",
    "        print (f'{train_dataset.indexer.index_to_word[l[i][0]]}: {l[i][1]}')\n",
    "    print('------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "KndtAs-QFdmU",
    "outputId": "992ee57c-c069-480f-f2ce-d86e43bb18e8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYDElEQVR4nO3dfXRV9Z3v8fd3iEIqDwFJy5Nc0CmshBACBApGpFgwTAeEqXQKK1QtbTPqBaujDDB0WVsfiuZOvcXiZWXWYHutME4RgqW1WAoVRBhJJDypeFHTsYHRoCY8JAqB7/0jIQV5SMjZJyfw+7zWylrZ++zz+333XqwPO9+9z9nm7oiIyKXvrxJdgIiItAwFvohIIBT4IiKBUOCLiARCgS8iEoikREzatWtX79OnTyKmFhG5aJWUlBxw99Tmvj8hgd+nTx+Ki4sTMbWIyEXLzP4Uy/vV0hERCYQCX0TkImdmfcxsV2PbKfBFRAKRkB6+iEjIHnzwQX75y1+SmprKVVddxdChQxk7diy333471dXVXHPNNSxZsoTOnTtTWlrasB64xsw6u/vHZjYUWFI/5ItNmVdn+CIiLWjr1q0899xzbN++nRdeeKHhBpZbbrmFRx99lB07djBw4EB++MMfnrEeqAF+UD/UU8Asdx/U1LkV+CIiLaBoWzk5C9aRO/df+fDKTH73xod06NCBiRMncuTIESorKxk9ejQAt956Kxs2bKCqquq09cCHwPVmlgKkuPuG+vVPN6UGtXREROKsaFs581bspObYcQAOfVLLvBU7W7wOneGLiMRZwZo9DWHftlcaNW+/SnVNDQueL2X16tVcccUVdO7cmY0bNwLw9NNPM3r0aDp16nTaeuBK4CV3rwQqzey6+vV5TalDZ/giInG2r7Km4fe23fuR/NfD2bdkJu9fkcLfDh9Ip06d+MUvftFwcfbqq6/mqaeeAjhtPZAM/Kh+qG8BS8zMaeJFW0vEA1Cys7Ndn7QVkVDkLFhH+Smhf+JoDX91eTLdPmfUPn8/hYWFDBkypNFxzKzE3bObW4fO8EVE4mx2bv/Tevgf/u5nHP/oPWqTjf+Z/+0mhX0UFPgiInE2eXBPoK6Xv6+yhqxb7md2bv+G9S1FgS8i0gImD+7Z4gH/WbpLR0QkEAp8EZFAKPBFRAKhwBcRCUQkgW9m481sj5ntNbO5UYwpIiLRijnwzawNsAj4GyAdmGZm6bGOKyIi0YriDH84sNfd33H3o8C/A5MiGFdERCIUReD3BN47ZfnP9etOY2b5ZlZsZsUVFRURTCsiIheixS7aunuhu2e7e3ZqampLTSsiIvWiCPxy4KpTlnvVrxMRkVYkisDfCnzRzPqa2eXAVOD5CMYVEZEIxfxdOu5ea2YzgTVAG2CJu++OuTIREYlUJF+e5u6/BX4bxVgiIhIf+qStiEggFPgiIoFQ4IuIBEKBLyISCAW+iEggFPgiIoFQ4IuIBEKBLyISCAW+iEggFPgiIoFQ4IuIBEKBLyISCAW+iEggFPgiIoFQ4IuIBEKBLyISCAW+iEggFPgiIoGIKfDN7OtmttvMTphZdlRFiYhI9GI9w98FfA3YEEEtIiISRzE9xNzd3wAws2iqERGRuFEPX0QkEI2e4ZvZWqDbWV6a7+6rmjqRmeUD+QC9e/ducoEiIhKNRgPf3cdGMZG7FwKFANnZ2R7FmCIi0nRq6YiIBCLW2zL/zsz+DIwEfmNma6IpS0REohbrXTorgZUR1SIiInGklo6ISCAU+CIigVDgi4gEQoEvIhIIBb6ISCAU+CIigVDgi4gEQoEvIhIIBb6ISCAU+CIigVDgi4gEQoEvIhIIBb6ISCAU+CIigVDgi4gEQoEvIhIIBb6ISCAU+CIigVDgi4gEItaHmBeY2ZtmtsPMVppZSlSFiYhItGI9w/89kOHumcBbwLzYSxIRkXiIKfDd/UV3r61f3AL0ir0kERGJhyh7+DOAF871opnlm1mxmRVXVFREOK2IiDRFUmMbmNlaoNtZXprv7qvqt5kP1ALPnGscdy8ECgGys7O9WdWKiEizNRr47j72fK+b2W3ABOAr7q4gFxFppRoN/PMxs/HAPwGj3b06mpJERCQeYu3h/wzoAPzezErNbHEENYmISBzEdIbv7n8dVSEiIhJf+qStiEggFPgiIoFQ4IuIBEKBLyISCAW+iEggFPgiIoFQ4IuIBEKBLyISCAW+iEggFPgiIoFQ4IuIBEKBLyISCAW+iEggFPgiIoFQ4IuIBEKBLyISCAW+iEggFPgiIoGIKfDN7EEz21H/PNsXzaxHVIWJiEi0Yj3DL3D3THfPAlYD90dQk4iIxEFMge/uB09ZvALw2MoREZF4SYp1ADN7GLgFqALGnGe7fCAfoHfv3rFOKyIiF8jcz39SbmZrgW5neWm+u686Zbt5QDt3/0Fjk2ZnZ3txcfGF1ioiEjQzK3H37Oa+v9EzfHcf28SxngF+CzQa+CIi0vJivUvni6csTgLejK0cERGJl1h7+AvMrD9wAvgTcHvsJYmISDzEFPjufnNUhYiISHzpk7YiIoFQ4IuIBEKBLyISCAW+iEggFPgiIoFQ4IuIBEKBLyISCAW+iEggFPgiIoFQ4IuIBEKBLyISCAW+iEggLsnALysrIyMjI6Yx/vjHP/LKK69EVJGISOJdkoEfBQW+iFxqLtnAr62tJS8vj7S0NKZMmUJ1dTUlJSWMHj2aoUOHkpuby/79+wFYuHAh6enpZGZmMnXqVMrKyli8eDGPP/44WVlZbNy4McF7IyISu0afaRsP8X6mbVlZGX379uXll18mJyeHGTNmkJaWxsqVK1m1ahWpqak8++yzrFmzhiVLltCjRw/effdd2rZtS2VlJSkpKTzwwAO0b9+e++67L251iohciLg/0/ZiUbStnII1e9hXWUMXr6Jrtx7k5OQAMH36dB555BF27drFuHHjADh+/Djdu3cHIDMzk7y8PCZPnszkyZMTtg8iIvF0SQR+0bZy5q3YSc2x4wC8f/ATKqtrKdpWzuTBPQHo0KEDAwYMYPPmzWe8/ze/+Q0bNmzg17/+NQ8//DA7d+5s0fpFRFpCJD18M7vXzNzMukYx3oUqWLOnIexPqj34AfcXrgBg6dKljBgxgoqKiobAP3bsGLt37+bEiRO89957jBkzhkcffZSqqioOHz5Mhw4dOHToUIvvi4hIvMQc+GZ2FXAj8F+xl9M8+yprzliX1KUX72xYQVpaGh9//DGzZs1i+fLlzJkzh0GDBpGVlcUrr7zC8ePHmT59OgMHDmTw4MHcddddpKSkMHHiRFauXKmLtiJyyYj5oq2ZLQceBFYB2e5+oLH3RH3RNmfBOsrPEvo9U5LZNPeGyOYREUmkWC/axnSGb2aTgHJ3396EbfPNrNjMiisqKmKZ9gyzc/uTfFmb09YlX9aG2bn9I51HRORi1uhFWzNbC3Q7y0vzgX+mrp3TKHcvBAqh7gz/Amps1MkLsyfv0umRkszs3P4N60VEpAmB7+5jz7bezAYCfYHtZgbQC3jNzIa7+39HWmUTTB7cUwEvInIezb4t0913Ap8/uWxmZTSxhy8iIi3vkv1qBREROV1kH7xy9z5RjSUiItHTGb6ISCAU+CIigVDgi4gEQoEvIhIIBb6ISCAU+CIigVDgi4gEQoEvIhIIBb6ISCAU+CIigVDgi4gEQoEvIhIIBb6ISCAU+CIigVDgi4gEQoEvIhIIBb6ISCAU+CIigYgp8M3sATMrN7PS+p+vRlWYXLqOHz+e6BJEghTFGf7j7p5V//PbCMaTVqygoICFCxcCcM8993DDDTcAsG7dOvLy8li2bBkDBw4kIyODOXPmNLyvffv23HvvvQwaNIjNmzczd+5c0tPTyczM5L777gOgoqKCm2++mWHDhjFs2DA2bdrU8jsocglTS0cuyKhRo9i4cSMAxcXFHD58mGPHjrFx40b69evHnDlzWLduHaWlpWzdupWioiIAjhw5wpe+9CW2b99OWloaK1euZPfu3ezYsYPvf//7AHzve9/jnnvuYevWrTz33HN85zvfSdh+ilyKogj8mWa2w8yWmFnnc21kZvlmVmxmxRUVFRFMGx9FRUW8/vrriS6j1SnaVk7OgnVMfe59fv2HTSzd+CZt27Zl5MiRFBcXs3HjRlJSUvjyl79MamoqSUlJ5OXlsWHDBgDatGnDzTffDECnTp1o164d3/72t1mxYgWf+9znAFi7di0zZ84kKyuLm266iYMHD3L48OGE7bPIpabRwDeztWa26yw/k4D/A1wDZAH7gX851zjuXuju2e6enZqaGtkONNe5+sgK/DMVbStn3oqdlFfWQJskrGMq9zz0U7pcncGoUaNYv349e/fupU+fPucco127drRp0waApKQkXn31VaZMmcLq1asZP348ACdOnGDLli2UlpZSWlpKeXk57du3b4ldFAlCo4Hv7mPdPeMsP6vc/X13P+7uJ4B/BYbHv+T49ZFfeeUVnn/+eWbPnk1WVhZvv/12S+xOq1ewZg81x/7yH2TbXgP4cPNz7D7Rk1GjRrF48WIGDx7M8OHDeemllzhw4ADHjx9n2bJljB49+ozxDh8+TFVVFV/96ld5/PHH2b59OwA33ngjTzzxRMN2paWl8d85kYDEepdO91MW/w7YFVs5TROvPvK1117LTTfdREFBAaWlpVxzzTUtsTut3r7KmtOW2/YawPEjH3G449V84QtfoF27dowaNYru3buzYMECxowZw6BBgxg6dCiTJk06Y7xDhw4xYcIEMjMzue666/jJT34CwMKFCykuLiYzM5P09HQWL17cIvsnEoqkGN//mJllAQ6UAf8Qc0XnULStnII1e9hXWUO3Dpfx7uZXOXjwIG3btmXIkCENfeSJEyc29JGBhj7y5MmTz9lHnjBhAhMmTIhX6a1OWVkZEyZMYNeupv3/nPT2S9R8PoOkDlcCcKziT1x197P0Sq27ZPPWW281bDtt2jSmTZt2xhin9uK7d+/Oq6++esY2Xbt25dlnn72gfRGRpovpDN/dv+nuA909091vcvf9URV2qlN7yA7sP3SMQ5d15h8f+t9ce+21kfaR5Uxt391I0ieVDcsHi1fRllpm5/Zv8hi6914k8S6K2zI/20MGuKxnOk8XLuL666+PtI/coUMHDh061CL7lUi1tbXk5eWRlpbGlClTqK6upqSkhNGjRzN06FByc3PZv38/y5cvp+zNnRz7w0+p+L/f41DxKk4c+Yijz/+An/7jdABefPFFRo4cyZAhQ/j617/ecDbfp08f5syZw5AhQ/jVr36VyN0VES6SwP9sDxnq+shHD33IyJEjI+0jT506lYKCAgYPHnxJX7Tds2cPd955J2+88QYdO3Zk0aJFzJo1i+XLl1NSUsKMGTOYP38+U6ZMITs7m1XLn+XIvr18uLaQ3r16UrL5ZdavX8+BAwd46KGHWLt2La+99hrZ2dkNxxLgyiuv5LXXXmPq1KkJ3FsRgdh7+C2iR0py3S2Bp0juk8W1D7/IFVdcAUTXR87Jybkkb8s89RpIF6+ia7ce5OTkADB9+nQeeeQRdu3axbhx44C6Fkz37t3PNyQAW7Zs4fXXX28Y6+jRo4wcObLh9W984xtx2BsRaY6LIvBn5/Zn3oqdp7V1ki9rc0E95JCdvAZy8vi9f/ATKqtrKdpWzuTBPYG6VtaAAQPYvHnzBY3t7owbN45ly5ad9fWT/yGLSOJdFC2dyYN78uOvDaRnSjIG9ExJ5sdfG9gQVnJ+Z7sGUnvwA+4vXAHA0qVLGTFiBBUVFQ2Bf+zYMXbv3g2ceV3j1OURI0awadMm9u7dC9Td+nrqX1si0npcFGf4UBf6CvjmOds1kKQuvXhnwwrS0p4kPT2dWbNmkZuby1133UVVVRW1tbXcfffdDBgwgNtuu43bb7+d5ORkNm/eTH5+PuPHj6dHjx6sX7+en//850ybNo1PP/0UgIceeoh+/fq19G6KSCPM3Vt80uzsbC8uLm7xeUOVs2DdGddAoO4vpU1zb0hARSLSHGZW4u7ZzX3/RdHSkdjMzu1P8mVtTlunayAi4bloWjrSfCdbYSfv0umRkszs3P5qkYkERoEfCF0DERG1dEREAqHAFxEJhAJfRCQQCnwRkUAo8EVEAqHAFxEJhAJfRCQQCnwRkUDEHPhmNsvM3jSz3Wb2WBRFiYhI9GL6pK2ZjQEmAYPc/VMz+3w0ZYmISNRiPcO/A1jg7p8CuPsHsZckIiLxEGvg9wNGmdl/mtlLZjYsiqJERCR6jbZ0zGwt0O0sL82vf38XYAQwDPgPM7vaz/Il+2aWD+QD9O7dO5aaRUSkGRoNfHcfe67XzOwOYEV9wL9qZieArkDFWcYpBAqh7gEoza5YRESaJdaWThEwBsDM+gGXAwdiLUpERKIX6/fhLwGWmNku4Chw69naOSIikngxBb67HwWmR1SLiIjEkT5pKyISCAW+iEggFPgiIoFQ4IuIBEKBLyISCAW+iEggFPgiIoFQ4IuIBEKBLyISCAW+iEggFPgiIjEoKysjIyOjxd/bHAp8EZFAKPBFRGJUW1tLXl4eaWlpTJkyherqan70ox8xbNgwMjIyyM/P5+QXCZeUlDBo0CAGDRrEokWLWrROBb6ISIz27NnDnXfeyRtvvEHHjh158sknmTlzJlu3bmXXrl3U1NSwevVqAL71rW/xxBNPsH379havM9bvwxcRCU7RtnIK1uxhX2UNXbyKrt16kJOTA8D06dNZuHAhffv25bHHHqO6upqPPvqIAQMGMGrUKCorK7n++usB+OY3v8kLL7zQYnXrDF9E5AIUbStn3oqdlFfW4MD7Bz+hsrqWom3lDduYGXfeeSfLly9n586dfPe73+WTTz5JXNH1FPgiIhegYM0eao4dP21d7cEPuL9wBQBLly7luuuuA6Br164cPnyY5cuXA5CSkkJKSgovv/wyAM8880wLVq6WjojIBdlXWXPGuqQuvXhnwwrS0p4kPT2dO+64g48//piMjAy6devGsGHDGrZ96qmnmDFjBmbGjTfe2JKlY4l4BG12drYXFxe3+LwiIrHKWbCO8rOEfs+UZDbNvSGuc5tZibtnN/f9MbV0zOxZMyut/ykzs9JYxhMRae1m5/Yn+bI2p61LvqwNs3P7J6iipov1IebfOPm7mf0LUBVzRSIirdjkwT0BGu7S6ZGSzOzc/g3rW7NIevhmZsDfA/H9e0ZEpBWYPLjnRRHwnxXVXTqjgPfd/f+dawMzyzezYjMrrqioiGhaERFpqkbP8M1sLdDtLC/Nd/dV9b9PA5adbxx3LwQKoe6i7QXWKSIiMWo08N197PleN7Mk4GvA0KiKEhGR6EXR0hkLvOnuf45gLBERiZMoAn8qjbRzREQk8RLywSszqwD+1OITn19X4ECii2jFdHzOTcfm/HR8zu1Cj83/cPfU5k6WkMBvjcysOJZPsF3qdHzOTcfm/HR8zq2lj42+PE1EJBAKfBGRQCjw/6Iw0QW0cjo+56Zjc346PufWosdGPXwRkUDoDF9EJBAKfBGRQCjwT2FmWWa2pf77/YvNbHiia2ptzGyWmb1pZrvN7LFE19PamNm9ZuZm1jXRtbQWZlZQ/29mh5mtNLOURNfUGpjZeDPbY2Z7zWxuS8ypwD/dY8AP3T0LuL9+WeqZ2RhgEjDI3QcA/yvBJbUqZnYVcCPwX4mupZX5PZDh7pnAW8C8BNeTcGbWBlgE/A2QDkwzs/R4z6vAP50DHet/7wTsS2AtrdEdwAJ3/xTA3T9IcD2tzePAP1H370jqufuL7l5bv7gF6JXIelqJ4cBed3/H3Y8C/07dyVRcKfBPdzdQYGbvUXf2GvyZyGf0A0aZ2X+a2UtmNqzRdwTCzCYB5e6+PdG1tHIzgBcSXUQr0BN475TlP9evi6tInnh1MTnf9/sDXwHucffnzOzvgX+j7ttAg9HI8UkCugAjgGHAf5jZ1R7Ivb2NHJt/pq6dE6SmPDfDzOYDtcAzLVmb/IXuwz+FmVUBKe7u9Y9trHL3jo29LxRm9jvgUXdfX7/8NjDC3YN+hJmZDQT+AFTXr+pFXTtwuLv/d8IKa0XM7DbgH4CvuHt1I5tf8sxsJPCAu+fWL88DcPcfx3NetXROtw8YXf/7DcA5H9kYqCJgDICZ9QMuR9+CiLvvdPfPu3sfd+9D3Z/nQxT2dcxsPHXXNm5S2DfYCnzRzPqa2eXUfc388/GeNLiWTiO+C/y0/ilenwD5Ca6ntVkCLDGzXcBR4NZQ2jkSk58BbYHf1/3hzBZ3vz2xJSWWu9ea2UxgDdAGWOLuu+M9r1o6IiKBUEtHRCQQCnwRkUAo8EVEAqHAFxEJhAJfRCQQCnwRkUAo8EVEAvH/Aawg3OEkeAapAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def project_terms(x,words,indexer):\n",
    "  tsne = TSNE(n_components=2).fit_transform(x)\n",
    "\n",
    "  x =[]; y=[]\n",
    "\n",
    "  for w in words:\n",
    "    x.append(tsne[indexer[w],0])\n",
    "    y.append(tsne[indexer[w],1])\n",
    "\n",
    "  plt.scatter(x,y)\n",
    "\n",
    "  for i in range(len(words)):\n",
    "    plt.annotate(words[i], (x[i], y[i]))\n",
    "\n",
    "project_terms(row_normalized,['good','bad','better','worse','best','worst'],train_dataset.indexer.word_to_index)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Glove_release.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
