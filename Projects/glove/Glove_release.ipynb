{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7k0j3i04EiT"
   },
   "source": [
    "# CSE 325/425 NLP\n",
    "### Programming Project 1\n",
    "\n",
    "You are asked to implement basic text pre-procssing and then define and train the Glove model. I removed parts of my implementation and you will need to complete them.\n",
    "\n",
    "Your codes are evaluated based on\n",
    "\n",
    "*   Correct and reasonable text preprocessing.\n",
    "*   Convergence of the model training.\n",
    "*   Speeding up of the word indexing and model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0HNfPb3gzHV8"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-315a36cb8182>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# the following working directory should contain small.csv and glove.6B.300d.txt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive/My Drive/Teaching/teaching at Lehigh/2021_sp_nlp/Project 1/data/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# the following working directory should contain small.csv and glove.6B.300d.txt\n",
    "os.chdir('/content/drive/My Drive/Teaching/teaching at Lehigh/2021_sp_nlp/Project 1/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbdCMdr6uxlS"
   },
   "source": [
    "## Text Preprocessing and Dataset Construction\n",
    "\n",
    "### Define the WordIndexer class to\n",
    "*   hold the mapping from words to their indices and the indices to words.\n",
    "*   map from a list of sentences to a list of integers so that words are mapped to their indices, in the same order as the original words (except some words replaced).\n",
    "\n",
    "### Inherit from the `torch.utils.data.Dataset` class and create the AmazonReviewGloveDataset class to\n",
    "\n",
    "\n",
    "*   load the Amazon reviews in the csv format. Tokenize the review texts into sentences (a review can contain more than one sentence).\n",
    "*   use the WordIndexer class to obtain the indices of the words in the sentences.\n",
    "*   compute the X (word co-occurrence) matrix as the Glove paper indicates.\n",
    "\n",
    "We provide the function to read the pretrained word vectors from text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFcyggrF0h-6"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class WordIndexer:\n",
    "    \"\"\"Transform a dataset of text to a list of index of words.\"\"\"\n",
    "\n",
    "    def __init__(self, min_word_occurences=10, oov_word=\"OOV\"):\n",
    "        \"\"\" min_word_occurrences: integer, the minimum frequency of the word to keep.\n",
    "            oov_word: string, a special string for out-of-vocabulary words.\n",
    "        \"\"\"\n",
    "        self.oov_word = oov_word\n",
    "        self.min_word_occurences = min_word_occurences\n",
    "        # word to integer index mapping\n",
    "        self.word_to_index = {oov_word: 0}\n",
    "        # the inverse of the above mapping\n",
    "        self.index_to_word = [oov_word]\n",
    "        # this is for storing the word frequencies for removing infrequent words\n",
    "        self.word_occurrences = {}\n",
    "        # regular expression for retaining meaningful English words\n",
    "        self.re_words = re.compile(r\"\\b[a-zA-Z]{2,}\\b\")\n",
    "\n",
    "    def get_word_index(self, word, add_new_word = True):\n",
    "        \"\"\" Find the index of a word.\n",
    "                \n",
    "            word: string, the query word.\n",
    "            add_new_word: if true and the word has no entry, assign a new integer index to word.\n",
    "                            if false, return the index of the oov_word\n",
    "        \"\"\"\n",
    "        ### Your codes go here (10 points) ###\n",
    "\n",
    "    @property\n",
    "    def n_words(self):\n",
    "        \"\"\" return: the vocabulary size\n",
    "        \"\"\"\n",
    "        return len(self.word_to_index)\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        \"\"\" texts: list of sentences, each of which is a string\n",
    "            \n",
    "            Split each sentence into a list of words.\n",
    "            Then filter out the infrequent words.\n",
    "            Other text preprocessing, such as\n",
    "                lower-casing,\n",
    "                stop-word removal, and\n",
    "                advance word tokenization\n",
    "                are possible here.\n",
    "            Lastly setup the word-to-index and index-to-word dictionaries.\n",
    "            \n",
    "            return: a list of lists of indices of words in each sentence.\n",
    "                    For example: [[1,2,3], [4,5,6]] where,\n",
    "                        [1,2,3] are the indices of words in the first sentence\n",
    "                        [4,5,6] are the indices of words in the second sentence\n",
    "                    \n",
    "        \"\"\"\n",
    "        \n",
    "        # Step 1: Obtain list of lists of words. Lower-casing and tokenization happen here.\n",
    "        ### Your codes go here (10 points) ###\n",
    "\n",
    "\n",
    "        # Step 2: Build a dictionary using the Counter class\n",
    "        # keep the unique words and their counts\n",
    "        # filter out the infrequent ones using the threshold self.min_word_occurences.\n",
    "        # the results is a vocabulary in self.word_to_index and self.index_to_word.\n",
    "        ### Your codes go here (10 points) ###\n",
    "\n",
    "\n",
    "        # save the word and their counts to a file.\n",
    "        with open('./train_word_counts.txt', 'w') as out_f:\n",
    "            a = sorted([(word, count) for word, count in word_occurrences.items()],\n",
    "                   key = lambda x:x[1], reverse=True)\n",
    "            for word, count in a:\n",
    "                out_f.write('{}:{}\\n'.format(word, count))\n",
    "\n",
    "        # Step 3: build and return the corpus in index representation\n",
    "        # using the vocabulary built in the last step.\n",
    "        # Be careful about words that are not in the vocabulary.\n",
    "        ### Your codes go here (10 points) ###\n",
    "    \n",
    "class AmazonReviewGloveDataset(Dataset):\n",
    "    def __init__(self, path, right_window = 4, min_word_occurences = 10):\n",
    "        \"\"\" Load the reviews from a csv file. One row is one review.\n",
    "                \n",
    "            path: path to the csv file containing the reviews and their ratings\n",
    "            right_window: integer, how large the window is to get context words.\n",
    "            min_word_occurrences: integer, the minimum frequency of the word to keep.\n",
    "\n",
    "            No return value\n",
    "        \"\"\"\n",
    "        self.right_window = right_window\n",
    "        \n",
    "        # Step 1: tokenize the first field of each row in the csv file into sentences\n",
    "        #         (e.g. using nltk.tokenize.sent_tokenize).\n",
    "        #           Use pandas.read_csv to load the given training csv file.\n",
    "        df = pd.read_csv(path)\n",
    "        texts = []  # each element of texts is a single sentence.\n",
    "        ### Your codes go here (10 points) ###\n",
    "        \n",
    "\n",
    "        print ('{} reviews loaded. {} sentences.'.format(df.shape[0], len(texts)))\n",
    "        \n",
    "        \n",
    "        # Step 2: pass the list of all sentences from step 1 (texts) to WordIndexer.\n",
    "        # Use its fit_transform function to turn list of sentences into list of lists of word indices in the sentences.\n",
    "        # Keep the word ordering.\n",
    "        print ('Indexing the corpus...')\n",
    "        self.indexer = WordIndexer(min_word_occurences=min_word_occurences)\n",
    "        corpus = self.indexer.fit_transform(texts, use_existing_indexer = False)\n",
    "        print ('Done indexing the corpus.')\n",
    "        \n",
    "        \n",
    "        # Step 3: go through the results (corpus) from step 2 and gather (center, context) in comatrix,\n",
    "        # which is a collections.Counter object.\n",
    "        # In the Counter, keys are (center, context) pairs\n",
    "        # values are the number of their co-occurrence as defined in the Glove paper.\n",
    "        print ('Constructing the co-occurrence matrix...')\n",
    "        comatrix = Counter()\n",
    "        ### Your codes go here (10 points) ###\n",
    "\n",
    "\n",
    "        # save the comatrix to file                    \n",
    "        with open('./comatrix.pkl', 'wb') as out_f:\n",
    "            pickle.dump(comatrix, out_f)\n",
    "\n",
    "        # Step 4: flatten the co-occurrence matrix and store the center, context, and X_ij\n",
    "        # in three lists: self.left (center word), self.right (context word), self.n_occurrences (X_ij)\n",
    "        self.left, self.right, self.n_occurrences = None, None, None\n",
    "        ### Your codes go here (10 points) ###\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.left[index], self.right[index], self.n_occurrences[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.left)\n",
    "    \n",
    "def load_pretrained_wv(path):\n",
    "    \"\"\"\n",
    "        Load the pretrained word vectors downloaded from Stanford NLP.\n",
    "    \"\"\"\n",
    "    wv = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            items = line.split(' ')\n",
    "            wv[items[0]] = torch.DoubleTensor([float(a) for a in items[1:]])\n",
    "    return wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ2q4gbfNHfT"
   },
   "source": [
    "## Define the Glove model\n",
    "The parameters include\n",
    "\n",
    "*   Vectors of words when used as center and context words\n",
    "\n",
    "The parameters are defined for you already and please don't change the variable names.\n",
    "\n",
    "There is an option to pass in pre-trained word vectors to replace random initialization of the word vectors in this model.\n",
    "\n",
    "You have to complete the forward function to compute the predictions of log X_ij.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lzv3KCwHBrVv"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.functional as F\n",
    "\n",
    "class GloveModel(nn.Module):\n",
    "    def __init__(self, word_indexer, wv = None, word_dims = 300, BASE_STD = 0.01, random_state = 0):\n",
    "        \"\"\" Specify and initialize the parameters of the Glove network.\n",
    "        \"\"\"\n",
    "        super(GloveModel, self).__init__()\n",
    "        num_words = word_indexer.n_words\n",
    "        \n",
    "        torch.manual_seed(random_state)\n",
    "        \n",
    "        # initialize left and right word vectors\n",
    "        self.L_vecs = (torch.randn((num_words, word_dims))  * BASE_STD)\n",
    "        self.R_vecs = (torch.randn((num_words, word_dims))  * BASE_STD)\n",
    "       \n",
    "        if wv is not None:\n",
    "            num_replaced = 0\n",
    "            for i in range(num_words):\n",
    "                word = word_indexer.index_to_word[i]\n",
    "                if word in wv:\n",
    "                    num_replaced += 1\n",
    "                    self.L_vecs[i] = wv[word]\n",
    "                    self.R_vecs[i] = wv[word]\n",
    "            print (f'Replaced {float(num_replaced) / num_words}')\n",
    "            \n",
    "        self.L_vecs.requires_grad_()\n",
    "        self.R_vecs.requires_grad_()\n",
    "        \n",
    "        # gather the trainable parameters\n",
    "        self.parameters = [self.L_vecs, self.R_vecs]\n",
    "        \n",
    "    def forward(self, left_indices, right_indices):\n",
    "        \"\"\" Implement w_i^t w_j (the left-hand-side of Eq. (16) in the Glove paper)\n",
    "        \n",
    "            left_indices: torch.Tensor, a batch of center words\n",
    "            right_indices: torch.Tensor, a batch of context words, of the same shape of left_indices.\n",
    "            \n",
    "            left_indices[i] and right_indices[i] is the i-th pair in the training data.\n",
    "            \n",
    "            return: torch.Tensor of the same shape of left_indices\n",
    "        \"\"\"\n",
    "        ### Your codes go here (10 points) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1goimxBiNRDi"
   },
   "source": [
    "## Model training, validating, and saving\n",
    "\n",
    "### First define some constants\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCvtEVViNF8D"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# this will automatically place all tensor on GPU with type Double.\n",
    "# if you are not running on GPU, change this line to\n",
    "# torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "torch.set_default_tensor_type('torch.cuda.DoubleTensor')\n",
    "\n",
    "# set up a couple of parameters and hyper-parameters\n",
    "\n",
    "# number of epoches to train the model\n",
    "NUM_EPOCH = 25\n",
    "# size of mini-batches\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# dimension of word vectors. The integer should be the same as the dimension of\n",
    "# pretrained word vectors.\n",
    "NUM_DIMS = 300\n",
    "\n",
    "# how to many words to the right to pair with the center word\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "# two hyper-parameters in Eq. (9) of the paper\n",
    "x_max = 100\n",
    "alpha = 0.75\n",
    "\n",
    "# input file containing Amazon review texts.\n",
    "train_path = './small.csv'\n",
    "\n",
    "# where your model is saved.\n",
    "save_path = './glove_model_{}.pt'\n",
    "\n",
    "# optional word vectors pretrained\n",
    "pretrained_wv = './glove.6B.{}d.txt'.format(NUM_DIMS)\n",
    "print (pretrained_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mn1Hz3QSqaCv"
   },
   "outputs": [],
   "source": [
    "# load pretrained word vectors\n",
    "wv = load_pretrained_wv(pretrained_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnFqhAwurmCt"
   },
   "outputs": [],
   "source": [
    "print (wv['good'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z39JUijCxi4i"
   },
   "source": [
    "### Then define the training, validation, and test data.\n",
    "*   Use the AmazonReviewGloveDataset class to read train dataset.\n",
    "*   Define DataLoader wrapping around the Dataset objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A77sOk99H-nv"
   },
   "outputs": [],
   "source": [
    "# load text data and turn them into a DataLoader object.\n",
    "train_dataset = AmazonReviewGloveDataset(train_path, right_window = WINDOW_SIZE)\n",
    "train_iter = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFfzclO7yOwU"
   },
   "source": [
    "### Third, start training.\n",
    "\n",
    "*   You're required to use GPU to train the network, since GPU are ubiquitous (colab or SandBox).\n",
    "*   Complete the function train_and_validate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBqcAinANLUn"
   },
   "outputs": [],
   "source": [
    "# decide whether to use cpu or gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# initialize the Glove model\n",
    "model = GloveModel(train_dataset.indexer, wv, word_dims = NUM_DIMS)\n",
    "\n",
    "# make sure you use weight_decay to activate the L2 regularization\n",
    "optimizer = torch.optim.Adam(model.parameters, weight_decay=1e-8)\n",
    "\n",
    "def train_and_validate(train_iter):\n",
    "    best_loss = -1\n",
    "    best_epoch = -1\n",
    "    to_save = {}\n",
    "    \n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = len(train_iter)\n",
    "        for l, r, n_lr in train_iter:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Implement the loss function in Eq. (16) of the paper, in three steps.\n",
    "            # Step 1. find the prediction of log(X_ij) using the model \n",
    "            ### Your codes go here (3 points) ###\n",
    "\n",
    "\n",
    "            # Step 2. compute the weights f(X_ij). See Eq. (9) of the Glove paper.\n",
    "            ### Your codes go here (3 points) ###\n",
    "\n",
    "\n",
    "            # Step 3. compute the loss in Eq. (16) using the predictions and the weights\n",
    "            ### Your codes go here (4 points) ###\n",
    "\n",
    "\n",
    "            # tracking the averaged loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # gradient descent, don't change the following two lines\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Training epoch = {epoch}, epoch loss = {epoch_loss / num_batches}')\n",
    "\n",
    "        # record the model state_dict() for saving later\n",
    "        to_save = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict()\n",
    "        }\n",
    "        torch.save(to_save, save_path.format(epoch))\n",
    "        print (save_path.format(epoch))\n",
    "    \n",
    "train_and_validate(train_iter, valid_iter = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPAs6x2kqUMY"
   },
   "source": [
    "## Retrieve similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWTYckiiqT3n"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "test_aspect_words = ['phone', 'case', 'battery', 'headset', 'charger', 'quality', 'screen', 'bluetooth', 'price', 'device']\n",
    "test_sentimental_words = ['great', 'good', 'well', 'works', 'better', 'little', 'easy', 'nice', 'new', 'long']\n",
    "\n",
    "glove = load_model(save_path.format(0), train_dataset.indexer)\n",
    "avg_word_vectors = (glove.L_vecs.to('cpu') + glove.R_vecs.to('cpu')) / 2\n",
    "avg_word_vectors = avg_word_vectors.detach().numpy()\n",
    "\n",
    "n_words = train_dataset.indexer.n_words\n",
    "\n",
    "row_normalized = normalize(avg_word_vectors)\n",
    "sim = row_normalized.dot(row_normalized.T)\n",
    "\n",
    "for w in test_aspect_words:\n",
    "    w_idx = train_dataset.indexer.word_to_index[w]\n",
    "    l = []\n",
    "    for i in range(n_words):\n",
    "        l.append((i, sim[w_idx, i]))\n",
    "    l = sorted(l, key = lambda x:x[1], reverse = True)\n",
    "    for i in range(10):\n",
    "        print (f'{train_dataset.indexer.index_to_word[l[i][0]]}: {l[i][1]}')\n",
    "        \n",
    "for w in test_sentimental_words:\n",
    "    w_idx = train_dataset.indexer.word_to_index[w]\n",
    "    l = []\n",
    "    for i in range(n_words):\n",
    "        l.append((i, sim[w_idx, i]))\n",
    "    l = sorted(l, key = lambda x:x[1], reverse = True)\n",
    "    for i in range(10):\n",
    "        print (f'{train_dataset.indexer.index_to_word[l[i][0]]}: {l[i][1]}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Glove (release).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
